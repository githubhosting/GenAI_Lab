{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cL624YiACjGG",
    "outputId": "1a60632c-59d5-43e0-9bb3-768b6fb300fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['the', 'sat', 'together', 'mat', 'and', 'on', 'dog', 'log', 'cat', 'played']\n",
      "TF-IDF Matrix:\n",
      " [[0.         0.         0.         0.08109302 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.08109302 0.         0.        ]\n",
      " [0.         0.         0.08109302 0.         0.08109302 0.\n",
      "  0.         0.         0.         0.08109302]]\n"
     ]
    }
   ],
   "source": [
    "# Question 1: Computing the TF-IDF Matrix using NumPy\n",
    "# Task: Write a Python function to compute the TF-IDF matrix for the given set of documents using only NumPy.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_tf_idf(documents, vocabulary):\n",
    "    N = len(documents)\n",
    "    V = len(vocabulary)\n",
    "\n",
    "    # Initialize TF matrix (N x V)\n",
    "    tf = np.zeros((N, V))\n",
    "\n",
    "    # Build term frequency matrix\n",
    "    for i, doc in enumerate(documents):\n",
    "        words = doc.lower().split()\n",
    "        for word in words:\n",
    "            if word in vocabulary:\n",
    "                j = vocabulary.index(word)\n",
    "                tf[i, j] += 1\n",
    "        tf[i] = tf[i] / len(words)  # Normalize TF by document length\n",
    "\n",
    "    # Compute Document Frequency (DF)\n",
    "    df = np.zeros(V)\n",
    "    for j, term in enumerate(vocabulary):\n",
    "        df[j] = sum(1 for doc in documents if term in doc.lower().split())\n",
    "\n",
    "    # Compute Inverse Document Frequency (IDF)\n",
    "    idf = np.log(N / (df + 1))  # Add 1 to avoid division by zero\n",
    "\n",
    "    # Compute TF-IDF matrix\n",
    "    tf_idf = tf * idf  # Element-wise multiplication\n",
    "\n",
    "    return tf_idf\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "documents = [\n",
    "    \"cat sat on the mat\",\n",
    "    \"dog sat on the log\",\n",
    "    \"cat and dog played together\"\n",
    "]\n",
    "\n",
    "vocabulary = list(set(\" \".join(documents).lower().split()))\n",
    "tf_idf_matrix = compute_tf_idf(documents, vocabulary)\n",
    "\n",
    "print(\"Vocabulary:\", vocabulary)\n",
    "print(\"TF-IDF Matrix:\\n\", tf_idf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fO8u7aejCrgQ",
    "outputId": "75e2dd47-376f-4e01-f891-8672ed4db0d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-grams:\n",
      "('the', 'quick', 'brown')\n",
      "('quick', 'brown', 'fox')\n",
      "('brown', 'fox', 'jumps')\n",
      "('fox', 'jumps', 'over')\n",
      "('jumps', 'over', 'the')\n",
      "('over', 'the', 'lazy')\n",
      "('the', 'lazy', 'dog.')\n"
     ]
    }
   ],
   "source": [
    "# Question 2: Generating n-grams for a Sentence\n",
    "# Task: Write a Python function to generate n-grams for a given sentence.\n",
    "\n",
    "\n",
    "def generate_ngrams(sentence, n):\n",
    "    words = sentence.lower().split()\n",
    "    ngrams = []\n",
    "    for i in range(len(words) - n + 1):\n",
    "        ngram = tuple(words[i:i + n])\n",
    "        ngrams.append(ngram)\n",
    "    return ngrams\n",
    "\n",
    "# Example usage:\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "n = 3\n",
    "ngrams = generate_ngrams(sentence, n)\n",
    "print(f\"{n}-grams:\")\n",
    "for gram in ngrams:\n",
    "    print(gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nrJ2mE6eCrcq",
    "outputId": "384bbc37-1c1e-4ce0-8c59-e7e27e8b236c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram Probabilities:\n",
      "('the', 'quick', 'brown'): 0.05\n",
      "('quick', 'brown', 'fox'): 0.05\n",
      "('brown', 'fox', 'jumps'): 0.05\n",
      "('fox', 'jumps', 'over'): 0.1\n",
      "('jumps', 'over', 'the'): 0.1\n",
      "('over', 'the', 'lazy'): 0.1\n",
      "('the', 'lazy', 'dog'): 0.1\n",
      "('the', 'quick', 'blue'): 0.05\n",
      "('quick', 'blue', 'fox'): 0.05\n",
      "('blue', 'fox', 'jumps'): 0.05\n",
      "('the', 'lazy', 'cat'): 0.05\n",
      "('lazy', 'dog', 'sleeps'): 0.05\n",
      "('dog', 'sleeps', 'under'): 0.05\n",
      "('sleeps', 'under', 'the'): 0.05\n",
      "('under', 'the', 'blue'): 0.05\n",
      "('the', 'blue', 'sky'): 0.05\n"
     ]
    }
   ],
   "source": [
    "# Question 3: Computing a 3-gram Language Model\n",
    "# Task: Write a Python function to compute a 3-gram language model.\n",
    "\n",
    "\n",
    "def compute_trigram_language_model(documents):\n",
    "    from collections import defaultdict\n",
    "\n",
    "    trigram_counts = defaultdict(int)\n",
    "    total_trigrams = 0\n",
    "\n",
    "    for doc in documents:\n",
    "        words = doc.lower().split()\n",
    "        for i in range(len(words) - 2):\n",
    "            trigram = tuple(words[i:i + 3])\n",
    "            trigram_counts[trigram] += 1\n",
    "            total_trigrams += 1\n",
    "\n",
    "    # Compute probabilities\n",
    "    trigram_probabilities = {}\n",
    "    for trigram, count in trigram_counts.items():\n",
    "        trigram_probabilities[trigram] = count / total_trigrams\n",
    "\n",
    "    return trigram_probabilities\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "documents = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"The quick blue fox jumps over the lazy cat\",\n",
    "    \"The lazy dog sleeps under the blue sky\"\n",
    "]\n",
    "\n",
    "trigram_model = compute_trigram_language_model(documents)\n",
    "\n",
    "print(\"Trigram Probabilities:\")\n",
    "for trigram, prob in trigram_model.items():\n",
    "    print(f\"{trigram}: {prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "plyHcfN4CraJ",
    "outputId": "2b373a5c-2aa9-46be-e589-d85d15ffb13f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'i': 0, 'love': 1, 'machine': 2, 'learning': 3, 'is': 4, 'amazing': 5, 'new': 6, 'things': 7}\n",
      "Embedding Matrix E:\n",
      " [[0.40231637 0.3346554  0.35442146]\n",
      " [0.68586788 0.31903106 0.72701013]\n",
      " [0.29908347 0.2140028  0.19369548]\n",
      " [0.88582751 0.40311595 0.67568947]\n",
      " [0.46743442 0.81240767 0.56726333]\n",
      " [0.05545807 0.30769949 0.93618876]\n",
      " [0.23429467 0.29777349 0.80632008]\n",
      " [0.50534808 0.67418682 0.41463167]]\n",
      "Embedding for 'learning': [0.88582751 0.40311595 0.67568947]\n",
      "Embedding for 'unknown': [0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Question 4: Creating a Word Embedding Matrix\n",
    "# Task:\n",
    "# 1. Implement the function create_embedding_matrix(corpus, embedding_dim).\n",
    "# 2. Test the function and get_word_vector with the given corpus and embedding_dim=3.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def create_embedding_matrix(corpus, embedding_dim):\n",
    "    # Preprocessing\n",
    "    vocabulary = {}\n",
    "    index = 0\n",
    "    for sentence in corpus:\n",
    "        words = sentence.lower().split()\n",
    "        for word in words:\n",
    "            if word not in vocabulary:\n",
    "                vocabulary[word] = index\n",
    "                index += 1\n",
    "\n",
    "    V = len(vocabulary)\n",
    "    # Initialize embedding matrix with random values between 0 and 1\n",
    "    E = np.random.rand(V, embedding_dim)\n",
    "\n",
    "    # Create word to index mapping (already done in vocabulary)\n",
    "    word_to_index = vocabulary\n",
    "\n",
    "    # Define get_word_vector function\n",
    "    def get_word_vector(word):\n",
    "        word = word.lower()\n",
    "        if word in word_to_index:\n",
    "            idx = word_to_index[word]\n",
    "            return E[idx]\n",
    "        else:\n",
    "            return np.zeros(embedding_dim)\n",
    "\n",
    "    return E, vocabulary, get_word_vector\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "corpus = [\n",
    "    \"I love machine learning\",\n",
    "    \"Machine learning is amazing\",\n",
    "    \"I love learning new things\"\n",
    "]\n",
    "embedding_dim = 3\n",
    "\n",
    "E, vocabulary, get_word_vector = create_embedding_matrix(corpus, embedding_dim)\n",
    "\n",
    "print(\"Vocabulary:\", vocabulary)\n",
    "print(\"Embedding Matrix E:\\n\", E)\n",
    "\n",
    "# Test get_word_vector\n",
    "word = \"learning\"\n",
    "vector = get_word_vector(word)\n",
    "print(f\"Embedding for '{word}':\", vector)\n",
    "\n",
    "# Test with a word not in the vocabulary\n",
    "word = \"unknown\"\n",
    "vector = get_word_vector(word)\n",
    "print(f\"Embedding for '{word}':\", vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GgcJZDpMCrXH",
    "outputId": "b7a540cc-0e96-4e5d-cff0-f3c291a4f36f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'i': 0, 'love': 1, 'machine': 2, 'learning': 3, 'is': 4, 'amazing': 5, 'new': 6, 'things': 7}\n",
      "Embedding Matrix E:\n",
      " [[0.90987928 0.64925575 0.55289113]\n",
      " [0.4        0.5        0.6       ]\n",
      " [0.1        0.2        0.3       ]\n",
      " [0.2        0.3        0.4       ]\n",
      " [0.38336576 0.33241192 0.57152138]\n",
      " [0.3        0.4        0.5       ]\n",
      " [0.53596045 0.17524967 0.65607087]\n",
      " [0.40380906 0.97700212 0.71761848]]\n",
      "Embedding for 'machine': [0.1 0.2 0.3]\n",
      "Embedding for 'i': [0.90987928 0.64925575 0.55289113]\n",
      "Embedding for 'unknown': [0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Question 5: Creating a Word Embedding Matrix with Pre-trained Embeddings\n",
    "# Task:\n",
    "# 1. Implement the function create_embedding_matrix_with_pretrained(corpus, pretrained_embeddings, embedding_dim).\n",
    "# 2. Test the function with the given corpus and pre-trained embeddings.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def create_embedding_matrix_with_pretrained(corpus, pretrained_embeddings, embedding_dim):\n",
    "    # Preprocessing\n",
    "    vocabulary = {}\n",
    "    index = 0\n",
    "    for sentence in corpus:\n",
    "        words = sentence.lower().split()\n",
    "        for word in words:\n",
    "            if word not in vocabulary:\n",
    "                vocabulary[word] = index\n",
    "                index += 1\n",
    "\n",
    "    V = len(vocabulary)\n",
    "    # Initialize embedding matrix\n",
    "    E = np.zeros((V, embedding_dim))\n",
    "\n",
    "    # Assign embeddings\n",
    "    for word, idx in vocabulary.items():\n",
    "        if word in pretrained_embeddings:\n",
    "            E[idx] = np.array(pretrained_embeddings[word])\n",
    "        else:\n",
    "            E[idx] = np.random.rand(embedding_dim)  # Random initialization\n",
    "\n",
    "    # Define get_word_vector function\n",
    "    def get_word_vector(word):\n",
    "        word = word.lower()\n",
    "        if word in vocabulary:\n",
    "            idx = vocabulary[word]\n",
    "            return E[idx]\n",
    "        else:\n",
    "            return np.zeros(embedding_dim)\n",
    "\n",
    "    return E, vocabulary, get_word_vector\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "corpus = [\n",
    "    \"I love machine learning\",\n",
    "    \"Machine learning is amazing\",\n",
    "    \"I love learning new things\"\n",
    "]\n",
    "\n",
    "pretrained_embeddings = {\n",
    "    \"machine\": [0.1, 0.2, 0.3],\n",
    "    \"learning\": [0.2, 0.3, 0.4],\n",
    "    \"amazing\": [0.3, 0.4, 0.5],\n",
    "    \"love\": [0.4, 0.5, 0.6]\n",
    "}\n",
    "\n",
    "embedding_dim = 3\n",
    "\n",
    "E, vocabulary, get_word_vector = create_embedding_matrix_with_pretrained(\n",
    "    corpus, pretrained_embeddings, embedding_dim)\n",
    "\n",
    "print(\"Vocabulary:\", vocabulary)\n",
    "print(\"Embedding Matrix E:\\n\", E)\n",
    "\n",
    "# Test get_word_vector\n",
    "word = \"machine\"\n",
    "vector = get_word_vector(word)\n",
    "print(f\"Embedding for '{word}':\", vector)\n",
    "\n",
    "word = \"i\"\n",
    "vector = get_word_vector(word)\n",
    "print(f\"Embedding for '{word}':\", vector)  # Randomly initialized\n",
    "\n",
    "word = \"unknown\"\n",
    "vector = get_word_vector(word)\n",
    "print(f\"Embedding for '{word}':\", vector)  # Returns zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qFO4sZ6vCrUU",
    "outputId": "888fc027-be66-4b41-f4e9-33731ca0d6eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'i': 0, 'love': 1, 'machine': 2, 'learning': 3, 'is': 4, 'amazing': 5, 'new': 6, 'things': 7}\n",
      "\n",
      "One-Hot Encodings:\n",
      "Word: 'i' - One-Hot Vector: [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Word: 'love' - One-Hot Vector: [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "Word: 'machine' - One-Hot Vector: [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "Word: 'learning' - One-Hot Vector: [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "Word: 'is' - One-Hot Vector: [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "Word: 'amazing' - One-Hot Vector: [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Word: 'new' - One-Hot Vector: [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "Word: 'things' - One-Hot Vector: [0. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Question 6: Generating One-Hot Encodings\n",
    "# Task:\n",
    "# 1. Implement the function create_one_hot_encodings(corpus).\n",
    "# 2. Test the function with the given corpus.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def create_one_hot_encodings(corpus):\n",
    "    # Preprocessing\n",
    "    vocabulary = {}\n",
    "    index = 0\n",
    "    for sentence in corpus:\n",
    "        words = sentence.lower().split()\n",
    "        for word in words:\n",
    "            if word not in vocabulary:\n",
    "                vocabulary[word] = index\n",
    "                index += 1\n",
    "\n",
    "    V = len(vocabulary)\n",
    "    # Initialize one-hot encoding matrix\n",
    "    one_hot_encodings = {}\n",
    "\n",
    "    for word, idx in vocabulary.items():\n",
    "        one_hot_vector = np.zeros(V)\n",
    "        one_hot_vector[idx] = 1\n",
    "        one_hot_encodings[word] = one_hot_vector\n",
    "\n",
    "    return vocabulary, one_hot_encodings\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "corpus = [\n",
    "    \"I love machine learning\",\n",
    "    \"Machine learning is amazing\",\n",
    "    \"I love learning new things\"\n",
    "]\n",
    "\n",
    "vocabulary, one_hot_encodings = create_one_hot_encodings(corpus)\n",
    "\n",
    "print(\"Vocabulary:\", vocabulary)\n",
    "print(\"\\nOne-Hot Encodings:\")\n",
    "for word, one_hot_vector in one_hot_encodings.items():\n",
    "    print(f\"Word: '{word}' - One-Hot Vector: {one_hot_vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J1D4NUxkD-es",
    "outputId": "17d81313-bf6e-4e7e-d41d-d3c2bb692e68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'i': 0, 'love': 1, 'machine': 2, 'learning': 3, 'is': 4, 'amazing': 5, 'new': 6, 'things': 7}\n",
      "\n",
      "Skip-Gram Training Pairs:\n",
      "('i', 'love')\n",
      "('i', 'machine')\n",
      "('love', 'i')\n",
      "('love', 'machine')\n",
      "('love', 'learning')\n",
      "('machine', 'i')\n",
      "('machine', 'love')\n",
      "('machine', 'learning')\n",
      "('learning', 'love')\n",
      "('learning', 'machine')\n",
      "('machine', 'learning')\n",
      "('machine', 'is')\n",
      "('learning', 'machine')\n",
      "('learning', 'is')\n",
      "('learning', 'amazing')\n",
      "('is', 'machine')\n",
      "('is', 'learning')\n",
      "('is', 'amazing')\n",
      "('amazing', 'learning')\n",
      "('amazing', 'is')\n",
      "('i', 'love')\n",
      "('i', 'learning')\n",
      "('love', 'i')\n",
      "('love', 'learning')\n",
      "('love', 'new')\n",
      "('learning', 'i')\n",
      "('learning', 'love')\n",
      "('learning', 'new')\n",
      "('learning', 'things')\n",
      "('new', 'love')\n",
      "('new', 'learning')\n",
      "('new', 'things')\n",
      "('things', 'learning')\n",
      "('things', 'new')\n"
     ]
    }
   ],
   "source": [
    "# Question 7: Implementing the Skip-Gram Model\n",
    "# Task:\n",
    "# 1. Implement the function generate_skip_gram_pairs(sentences, window_size).\n",
    "# 2. Test it with the given sentences and window_size = 2.\n",
    "\n",
    "\n",
    "def generate_skip_gram_pairs(sentences, window_size):\n",
    "    # Preprocessing: Build the vocabulary and word indices\n",
    "    vocabulary = {}\n",
    "    index = 0\n",
    "    for sentence in sentences:\n",
    "        words = sentence.lower().split()\n",
    "        for word in words:\n",
    "            if word not in vocabulary:\n",
    "                vocabulary[word] = index\n",
    "                index += 1\n",
    "\n",
    "    # Generate skip-gram training pairs\n",
    "    training_pairs = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.lower().split()\n",
    "        for i, target_word in enumerate(words):\n",
    "            # Define the context window\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(words), i + window_size + 1)\n",
    "            for j in range(start, end):\n",
    "                if i != j:\n",
    "                    context_word = words[j]\n",
    "                    training_pairs.append((target_word, context_word))\n",
    "\n",
    "    return vocabulary, training_pairs\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "sentences = [\n",
    "    \"I love machine learning\",\n",
    "    \"Machine learning is amazing\",\n",
    "    \"I love learning new things\"\n",
    "]\n",
    "\n",
    "window_size = 2\n",
    "\n",
    "vocabulary, training_pairs = generate_skip_gram_pairs(sentences, window_size)\n",
    "\n",
    "print(\"Vocabulary:\", vocabulary)\n",
    "print(\"\\nSkip-Gram Training Pairs:\")\n",
    "for pair in training_pairs:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t2AskN5zD-bF",
    "outputId": "b8f1483f-5a2b-400d-b96d-1f1898fa2bfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'i': 0, 'love': 1, 'machine': 2, 'learning': 3, 'is': 4, 'amazing': 5, 'new': 6, 'things': 7}\n",
      "\n",
      "CBOW Training Pairs:\n",
      "Context: ('love', 'machine'), Target: i\n",
      "Context: ('i', 'machine', 'learning'), Target: love\n",
      "Context: ('i', 'love', 'learning'), Target: machine\n",
      "Context: ('love', 'machine'), Target: learning\n",
      "Context: ('learning', 'is'), Target: machine\n",
      "Context: ('machine', 'is', 'amazing'), Target: learning\n",
      "Context: ('machine', 'learning', 'amazing'), Target: is\n",
      "Context: ('learning', 'is'), Target: amazing\n",
      "Context: ('love', 'learning'), Target: i\n",
      "Context: ('i', 'learning', 'new'), Target: love\n",
      "Context: ('i', 'love', 'new', 'things'), Target: learning\n",
      "Context: ('love', 'learning', 'things'), Target: new\n",
      "Context: ('learning', 'new'), Target: things\n"
     ]
    }
   ],
   "source": [
    "# Question 8: Generating CBOW Training Pairs\n",
    "# Task:\n",
    "# 1. Implement the function generate_cbow_pairs(sentences, window_size).\n",
    "# 2. Test it with the given sentences and window_size = 2.\n",
    "\n",
    "\n",
    "def generate_cbow_pairs(sentences, window_size):\n",
    "    # Preprocessing: Build the vocabulary and word indices\n",
    "    vocabulary = {}\n",
    "    index = 0\n",
    "    for sentence in sentences:\n",
    "        words = sentence.lower().split()\n",
    "        for word in words:\n",
    "            if word not in vocabulary:\n",
    "                vocabulary[word] = index\n",
    "                index += 1\n",
    "\n",
    "    # Generate CBOW training pairs\n",
    "    training_pairs = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.lower().split()\n",
    "        for i, target_word in enumerate(words):\n",
    "            # Define the context window\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(words), i + window_size + 1)\n",
    "            context_words = []\n",
    "            for j in range(start, end):\n",
    "                if i != j:\n",
    "                    context_words.append(words[j])\n",
    "            if context_words:\n",
    "                training_pairs.append((tuple(context_words), target_word))\n",
    "\n",
    "    return vocabulary, training_pairs\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "sentences = [\n",
    "    \"I love machine learning\",\n",
    "    \"Machine learning is amazing\",\n",
    "    \"I love learning new things\"\n",
    "]\n",
    "\n",
    "window_size = 2\n",
    "\n",
    "vocabulary, training_pairs = generate_cbow_pairs(sentences, window_size)\n",
    "\n",
    "print(\"Vocabulary:\", vocabulary)\n",
    "print(\"\\nCBOW Training Pairs:\")\n",
    "for pair in training_pairs:\n",
    "    print(f\"Context: {pair[0]}, Target: {pair[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0FsYFLKD-YM",
    "outputId": "b9460b44-127a-4221-c209-5d45be16ce62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs at each time step:\n",
      "Time step 1: y = [-0.00050584]\n",
      "Time step 2: y = [-0.00101643]\n",
      "Time step 3: y = [-0.00152624]\n"
     ]
    }
   ],
   "source": [
    "# Question 9: Implementing a Simple Vanilla RNN\n",
    "# Task:\n",
    "# 1. Implement the function rnn_forward(x, Wxh, Whh, Why, bh, by, h0).\n",
    "# 2. Test the function with random weights, biases, and an initial hidden state.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def rnn_forward(x, Wxh, Whh, Why, bh, by, h0):\n",
    "    h = h0\n",
    "    hs = []\n",
    "    ys = []\n",
    "    for t in range(len(x)):\n",
    "        xt = np.array([[x[t]]])  # Input at time t (make it a column vector)\n",
    "        h = np.tanh(np.dot(Whh, h) + np.dot(Wxh, xt) + bh)  # Hidden state\n",
    "        y = np.dot(Why, h) + by  # Output\n",
    "        hs.append(h)\n",
    "        ys.append(y)\n",
    "    return ys, hs\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Input sequence\n",
    "x = [1, 2, 3]\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 1   # Since x is a sequence of numbers\n",
    "hidden_size = 4  # You can choose any size for hidden state\n",
    "output_size = 1  # Output is a single number at each time step\n",
    "\n",
    "# Random initialization of weights and biases\n",
    "np.random.seed(0)  # For reproducibility\n",
    "Wxh = np.random.randn(hidden_size, input_size) * 0.01\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "Why = np.random.randn(output_size, hidden_size) * 0.01\n",
    "bh = np.zeros((hidden_size, 1))\n",
    "by = np.zeros((output_size, 1))\n",
    "h0 = np.zeros((hidden_size, 1))\n",
    "\n",
    "# Run the RNN forward function\n",
    "ys, hs = rnn_forward(x, Wxh, Whh, Why, bh, by, h0)\n",
    "\n",
    "print(\"Outputs at each time step:\")\n",
    "for t, y in enumerate(ys):\n",
    "    print(f\"Time step {t+1}: y = {y.flatten()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xLqJllcYD-U-",
    "outputId": "2c950286-434e-4d73-feaa-0ea61b3a5624"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Matrix X:\n",
      "[[0.5488135  0.71518937 0.60276338]\n",
      " [0.54488318 0.4236548  0.64589411]\n",
      " [0.43758721 0.891773   0.96366276]\n",
      " [0.38344152 0.79172504 0.52889492]]\n",
      "\n",
      "Weight Matrix Wq:\n",
      "[[0.56804456 0.92559664]\n",
      " [0.07103606 0.0871293 ]\n",
      " [0.0202184  0.83261985]]\n",
      "\n",
      "Weight Matrix Wk:\n",
      "[[0.77815675 0.87001215]\n",
      " [0.97861834 0.79915856]\n",
      " [0.46147936 0.78052918]]\n",
      "\n",
      "Weight Matrix Wv:\n",
      "[[0.11827443 0.63992102]\n",
      " [0.14335329 0.94466892]\n",
      " [0.52184832 0.41466194]]\n",
      "\n",
      "Self-Attention Output:\n",
      "[[0.53569849 1.29450415]\n",
      " [0.53551973 1.29413435]\n",
      " [0.53849796 1.29925955]\n",
      " [0.53131543 1.28657939]]\n"
     ]
    }
   ],
   "source": [
    "# Question 10 : Implementation of the self-attention mechanism using only NumPy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    \"\"\"Compute the softmax of each element along the specified axis of x.\"\"\"\n",
    "    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "def self_attention(X, Wq, Wk, Wv):\n",
    "    \"\"\"\n",
    "    Implement the self-attention mechanism.\n",
    "\n",
    "    Args:\n",
    "        X: Input matrix of shape (n, d), where n is the number of input vectors, and d is the dimension of each vector.\n",
    "        Wq: Query weight matrix of shape (d, dout).\n",
    "        Wk: Key weight matrix of shape (d, dout).\n",
    "        Wv: Value weight matrix of shape (d, dout).\n",
    "\n",
    "    Returns:\n",
    "        Output matrix of shape (n, dout).\n",
    "    \"\"\"\n",
    "    # Compute Queries (Q), Keys (K), and Values (V)\n",
    "    Q = np.dot(X, Wq)  # Shape: (n, dout)\n",
    "    K = np.dot(X, Wk)  # Shape: (n, dout)\n",
    "    V = np.dot(X, Wv)  # Shape: (n, dout)\n",
    "\n",
    "    # Compute attention scores: Q * K.T, then scale by sqrt(dout)\n",
    "    d_k = Q.shape[1]  # dout\n",
    "    attention_scores = np.dot(Q, K.T) / np.sqrt(d_k)  # Shape: (n, n)\n",
    "\n",
    "    # Apply softmax to attention scores\n",
    "    attention_weights = softmax(attention_scores, axis=-1)  # Shape: (n, n)\n",
    "\n",
    "    # Compute final output: Attention weights * V\n",
    "    output = np.dot(attention_weights, V)  # Shape: (n, dout)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "np.random.seed(0)  # For reproducibility\n",
    "\n",
    "# Input matrix X (n=4 vectors, d=3 features per vector)\n",
    "X = np.random.rand(4, 3)  # Shape: (4, 3)\n",
    "\n",
    "# Learnable weight matrices Wq, Wk, Wv\n",
    "d = 3      # Input dimension\n",
    "dout = 2   # Output dimension\n",
    "Wq = np.random.rand(d, dout)  # Shape: (3, 2)\n",
    "Wk = np.random.rand(d, dout)  # Shape: (3, 2)\n",
    "Wv = np.random.rand(d, dout)  # Shape: (3, 2)\n",
    "\n",
    "# Call the self_attention function\n",
    "output = self_attention(X, Wq, Wk, Wv)\n",
    "\n",
    "print(\"Input Matrix X:\")\n",
    "print(X)\n",
    "print(\"\\nWeight Matrix Wq:\")\n",
    "print(Wq)\n",
    "print(\"\\nWeight Matrix Wk:\")\n",
    "print(Wk)\n",
    "print(\"\\nWeight Matrix Wv:\")\n",
    "print(Wv)\n",
    "print(\"\\nSelf-Attention Output:\")\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
